{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCA CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"data csv with titles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Columns to drop\n",
    "    columns_to_drop = [\n",
    "        'WGT_KG_TCR', 'HGT_CM_TCR', 'BMI_TCR', 'PERM_STATE_TRR', 'PRI_PAYMENT_TRR', 'PRI_PAYMENT_CTRY_TRR',\n",
    "        'PX_STAT_TRR', 'PX_STAT_DATE_TRR', 'COD', 'PX_STAT_DATE', 'HBV_CORE_DON', 'HBV_SUR_ANTIGEN_DON',\n",
    "        'HCV_ANTIBODY_DON', 'HBV_NAT_DON', 'HCV_NAT_DON', 'HIV_NAT_DON', 'LIV_DON_TY', 'NON_HRT_DON', \n",
    "        'TXHRT', 'TXLNG', 'VCA_TY_MULTI', 'REGION', 'PREV_TX_ANY_N', 'PREV_TX', 'RETXDATE', 'INO_PROCURE_AGENT_3', \n",
    "        'HBSAB_DON', 'EBV_IGG_CAD_DON', 'EBV_IGM_CAD_DON', 'ECD_DONOR', 'CDC_RISK_HIV_DON', 'RECOV_COUNTRY_DON', \n",
    "        'RECOV_OUT_US_DON', 'CONTROLLED_DON', 'REFERRAL_DATE', 'RESUSCIT_DUR_DON', 'EDUCATION_DON', \n",
    "        'LT_ONE_WEEK_DON', 'ADMIT_DATE_DON', 'DONOR_ID', 'VCA_CANDIDATE_ID_CODE', 'EDUCATION', 'WORK_INCOME', \n",
    "        'WORK_YES_STATUS', 'WORK_NO_STATUS', 'GRANT_FUNDING', 'INSTITUTIONAL_FUNDING', 'SECONDARY_PAY', \n",
    "        'UPPER_LIMB_LEFT_AMP_LEVEL', 'UPPER_LIMB_RIGHT_AMP_LEVEL', 'HOSP_90_DAYS', 'VENTILATOR', 'OTH_LIFE_SUP', \n",
    "        'HBV_CORE', 'HBV_SUR_ANTIGEN', 'HCV_SEROSTATUS', 'PREV_PREG', 'MALIG', 'MALIG_TY_SKINMELANOMA', \n",
    "        'MALIG_TY_SKINNONMELANOMA', 'MALIG_TY_CNSTUMOR', 'MALIG_TY_GENITOURINARY', 'MALIG_TY_BREAST', \n",
    "        'MALIG_TY_THYROID', 'MALIG_TY_TONGUETHROATLARYNX', 'MALIG_TY_LUNG', 'MALIG_TY_LEUKEMIALYMPHOMA', \n",
    "        'MALIG_TY_LIVER', 'MALIG_TY_HEPATOCELLULARCARCINOMA', 'MALIG_TY_OTHERSPECIFY', 'COAGULOPATHIES', \n",
    "        'COGNITIVE_DEV', 'MOTOR_DEV', 'DASH_SCORE', 'CARROLL_SCORE_LEFT', 'CARROLL_SCORE_RIGHT', \n",
    "        'PHYSICAL_FUNC_SCORE', 'ROLE_PHYS_SCORE', 'BODILY_PAIN_SCORE', 'GENERAL_HEALTH_SCORE', 'VITALITY_SCORE', \n",
    "        'SOCIAL_FUNC_SCORE', 'ROLE_EMOTIONAL_SCORE', 'MENTAL_HEALTH_SCORE', 'WARM_ISCH_TM_LEFT', \n",
    "        'COLD_ISCH_TM_LEFT', 'WARM_ISCH_TM_RIGHT', 'COLD_ISCH_TM_RIGHT', 'ISCHEMIA_LEFT', 'ISCHEMIA_RIGHT', \n",
    "        'DISCH_SERUM_CREAT', 'DISCH_HEMOGLOBIN_A1C', 'CRANIOFACIAL_AMT_TISSUE_LOSS', 'WARM_ISCH_TM', \n",
    "        'COLD_ISCH_TM', 'ISCHEMIA', 'ABDOMINAL_WALL', 'LOWER_LIMB_LEFT_AMP_LEVEL', 'LOWER_LIMB_RIGHT_AMP_LEVEL', \n",
    "        'INIT_DATE', 'REM_CD', 'DEATH_DATE', 'END_DATE', 'COD_WL', 'INIT_HGT_CM', 'INIT_WGT_KG', 'INIT_AGE', \n",
    "        'DAYSWAIT_CHRON', 'INIT_REGION', 'INIT_CPRA', 'SKIN_TY', 'WLPI', 'WLPA', 'WLKP', 'WLKI', 'WLLI', 'WLIN', \n",
    "        'WLLU', 'WLHR', 'WLHL', 'LIST_YEAR', 'WL_ID_CODE', 'VCA_TY_CD', 'ETHNICITY', 'BODY_PART', \n",
    "        'TRANSPLANT_TIME', 'TRANSPLANTTIMEZONEID', 'DATA_TRANSPLANT', 'DATA_WAITLIST', 'CTR_CODE', \n",
    "        'OPO_CTR_CODE', 'INIT_OPO_CTR_CODE', 'END_OPO_CTR_CODE', 'LISTING_CTR_CODE', 'FORM_STATUS', \n",
    "        'FORM_VALID_DT', 'TX_YEAR', 'PT_CODE', 'SHARE_TY',\n",
    "        # Newly added columns to drop\n",
    "        'RA1', 'RA2', 'RB1', 'RB2', 'RDR1', 'RDR2', 'CPRA_TX', 'END_CPRA', 'ABO_MAT', \n",
    "        'HLAMIS', 'AMIS', 'BMIS', 'DRMIS', 'COMPOSITE_DEATH_DATE', 'HEMOGLOBIN_A1C', \n",
    "        'DONOR_CROSSMATCH', 'LIFE_SUP_TRR', 'MED_COND_TRR', 'TX_DATE', 'RECOVERY_DATE_DON', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'PX_STAT'\n",
    "    ]\n",
    "\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    df.replace(\".\", np.nan, inplace=True)\n",
    "\n",
    "    df['PTIME'] = pd.to_numeric(df['PTIME'], errors='coerce')\n",
    "    df['5_year'] = df['PTIME'] >= 1825 # It also presents false when dealing with missing PTIME values, so delete the missing rows.\n",
    "\n",
    "    float32_columns = [\n",
    "        'BMI_RECIP', 'HGT_CM_TRR', 'WGT_KG_TRR', 'PTIME', 'DISTANCE', 'BMI_DON_CALC', \n",
    "        'WGT_KG_DON_CALC', 'HGT_CM_DON_CALC'\n",
    "    ]\n",
    "\n",
    "    float16_columns = ['AGE', 'AGE_DON', 'SERUM_CREAT']\n",
    "\n",
    "    # Convert column types\n",
    "    df[float32_columns] = df[float32_columns].astype('float32')\n",
    "    df[float16_columns] = df[float16_columns].astype('float16')\n",
    "\n",
    "    # Convert DA1, DA2, DB1, DB2, DDR1, DDR2 to float\n",
    "    float_columns = ['DA1', 'DA2', 'DB1', 'DB2', 'DDR1', 'DDR2']\n",
    "    df[float_columns] = df[float_columns].apply(pd.to_numeric, errors='coerce', downcast='integer')\n",
    "    float_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[float_columns] = float_imputer.fit_transform(df[float_columns])\n",
    "\n",
    "    # Identify columns for KNN imputation\n",
    "    continuous_columns = ['HGT_CM_TRR', 'WGT_KG_TRR', 'AGE', 'WGT_KG_DON_CALC', 'HGT_CM_DON_CALC', 'SERUM_CREAT']\n",
    "    categorical_columns = ['ABO', 'ABO_DON']\n",
    "\n",
    "    # Impute categorical variables with the most frequent value\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "    # Apply KNN Imputation on continuous variables\n",
    "    continuous_imputer = KNNImputer(n_neighbors=5)\n",
    "    df[continuous_columns] = continuous_imputer.fit_transform(df[continuous_columns])\n",
    "\n",
    "    df['BMI_RECIP'] = df['BMI_RECIP'].fillna(df['WGT_KG_TRR'] / ((df['HGT_CM_TRR'] / 100) ** 2))\n",
    "    df['BMI_DON_CALC'] = df['BMI_DON_CALC'].fillna(df['WGT_KG_DON_CALC'] / ((df['HGT_CM_DON_CALC'] / 100) ** 2))\n",
    "\n",
    "\n",
    "    df.loc[df['ETHCAT'] == 998, 'ETHCAT'] = 3  # Replaces 998 (the unknown variable) with 3 for simplicity\n",
    "    df = df.fillna({'DIAG': '9'})  # Fills the diagnosis variable's missing values with \"9\" meaning none\n",
    "    df = df.fillna({'DISTANCE': df['DISTANCE'].median()})  # Fills missing values in distance with the median\n",
    "    df = df.fillna({'AGE': df['AGE'].median()})  # Fills missing values in age with the median\n",
    "    df.loc[df['INO_PROCURE_AGENT_1'] == '999', 'INO_PROCURE_AGENT_1'] = '7'  # Replaces the 999 values with 7\n",
    "    df.loc[df['INO_PROCURE_AGENT_2'] == '999', 'INO_PROCURE_AGENT_2'] = '7'  # Replaces the 999 values with 7\n",
    "    df = df.fillna({'GENDER_DON': df['GENDER_DON'].ffill()})  # Fills missing values using forward propagation\n",
    "    df = df.fillna({'ETHCAT_DON': df['ETHCAT_DON'].mode()[0]})  # Fills missing values with the mode\n",
    "    df = df.fillna({'CMV_STATUS': '4'})  # Fills missing values in CMV_STATUS with 4\n",
    "    df = df.fillna({'EBV_SEROSTATUS': '4'})  # Fills missing values in CMV_STATUS with 4\n",
    "    df = df.fillna({'AGE_DON': df['AGE_DON'].median()})  # Fills missing values with the median\n",
    "    \n",
    "    fill_u_columns = [\n",
    "        'ARGININE_DON', 'INSULIN_DON', 'TATTOOS_DON', 'PROTEIN_URINE_DON', \n",
    "        'INOTROP_SUPPORT_DON', 'TOLER_IND_TECH', 'PRE_TX_TXFUS', 'OTH_RISK_FACTORS'\n",
    "    ]\n",
    "    df.fillna({col: 'U' for col in fill_u_columns}, inplace=True)\n",
    "\n",
    "    category_columns = [\n",
    "        'ABO', 'ETHCAT', 'GENDER', 'DIAG', \n",
    "        'DON_TY', 'MULTIORG', 'TXINT', 'TXKID', 'TXLIV', 'TXPAN', 'TXVCA', 'MULTIVCA', \n",
    "        'PREV_TX_ANY', 'VCA_TY', 'PSTATUS', 'INO_PROCURE_AGENT_1', 'INO_PROCURE_AGENT_2',\n",
    "        'ARGININE_DON', 'INSULIN_DON', 'TATTOOS_DON', 'PROTEIN_URINE_DON', \n",
    "        'CARDARREST_POSTNEURO_DON', 'INOTROP_SUPPORT_DON', 'ABO_DON', 'GENDER_DON', \n",
    "        'ETHCAT_DON', 'CMV_STATUS', 'EBV_SEROSTATUS', 'TOLER_IND_TECH', 'PRE_TX_TXFUS', \n",
    "        'OTH_RISK_FACTORS', 'EXTRA_ALLOGRAFT'\n",
    "    ]\n",
    "    df[category_columns] = df[category_columns].astype('category')\n",
    "\n",
    "    # Ensure 'N' is a valid category before filling\n",
    "    columns_to_fill_N = ['MULTIORG', 'TXINT', 'TXKID', 'TXLIV', 'TXPAN', 'TXVCA', 'MULTIVCA', 'CARDARREST_POSTNEURO_DON', 'EXTRA_ALLOGRAFT']\n",
    "    for col in columns_to_fill_N:\n",
    "        if 'N' not in df[col].cat.categories:\n",
    "            df[col] = df[col].cat.add_categories(['N'])\n",
    "\n",
    "    # Fill missing values with 'N' for the specified columns\n",
    "    df.fillna({col: 'N' for col in columns_to_fill_N}, inplace=True)\n",
    "\n",
    "    # Ensure '0' is a valid category before filling\n",
    "    columns_to_fill_zero = ['INO_PROCURE_AGENT_1', 'INO_PROCURE_AGENT_2']\n",
    "    for col in columns_to_fill_zero:\n",
    "        if '0' not in df[col].cat.categories:\n",
    "            df[col] = df[col].cat.add_categories(['0'])\n",
    "\n",
    "    # Fill missing values with '0' for the specified columns\n",
    "    df.fillna({col: '0' for col in columns_to_fill_zero}, inplace=True)\n",
    "\n",
    "        # Binary encoding for Y/N variables\n",
    "    binary_columns_yn = [\n",
    "        'MULTIORG', 'TXVCA', 'MULTIVCA', 'PREV_TX_ANY', 'CARDARREST_POSTNEURO_DON', 'EXTRA_ALLOGRAFT'\n",
    "    ]\n",
    "    df[binary_columns_yn] = df[binary_columns_yn].map(lambda x: True if x == 'Y' else False)\n",
    "\n",
    "    # Binary encoding for special binary variables\n",
    "    binary_special_mappings = {\n",
    "        'DON_TY': {'C': True, 'L': False},\n",
    "        'TXINT': {'W': True, 'N': False},\n",
    "        'TXKID': {'E': True, 'N': False},\n",
    "        'TXLIV': {'W': True, 'N': False},\n",
    "        'TXPAN': {'W': True, 'N': False},\n",
    "        'PSTATUS': {1: True, 0: False},\n",
    "        'GENDER_DON': {'M': True, 'F': False},\n",
    "        'GENDER': {'M': True, 'F': False}\n",
    "    }\n",
    "\n",
    "    for col, mapping in binary_special_mappings.items():\n",
    "        df[col] = df[col].map(mapping)\n",
    "\n",
    "    # One-hot encoding for multi-category columns\n",
    "    one_hot_columns = [\n",
    "        'ABO', 'ETHCAT', 'DIAG', 'VCA_TY', \n",
    "        'INO_PROCURE_AGENT_1', 'INO_PROCURE_AGENT_2', 'ARGININE_DON', 'INSULIN_DON', \n",
    "        'TATTOOS_DON', 'PROTEIN_URINE_DON', 'INOTROP_SUPPORT_DON', 'ABO_DON', 'ETHCAT_DON', \n",
    "        'CMV_STATUS', 'EBV_SEROSTATUS', 'TOLER_IND_TECH', 'PRE_TX_TXFUS', 'OTH_RISK_FACTORS'\n",
    "    ]\n",
    "\n",
    "    df = pd.get_dummies(df, columns=one_hot_columns, drop_first=False)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "rej = pd.read_csv(\"REJECTIONS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRR_ID_CODE</th>\n",
       "      <th>ORGAN</th>\n",
       "      <th>DID_REJECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A795333</td>\n",
       "      <td>Scalp</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A795335</td>\n",
       "      <td>Upper Limb Right</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A795337</td>\n",
       "      <td>Uterus</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A795340</td>\n",
       "      <td>Abdominal Wall</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A795344</td>\n",
       "      <td>Penis</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TRR_ID_CODE             ORGAN DID_REJECT\n",
       "0     A795333             Scalp          N\n",
       "1     A795335  Upper Limb Right          Y\n",
       "2     A795337            Uterus          Y\n",
       "3     A795340    Abdominal Wall          N\n",
       "4     A795344             Penis          Y"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(rej):\n",
    "    # Rename column 'TRR ID' to 'TRR_ID_CODE'\n",
    "    rej = rej.rename(columns={'TRR ID': 'TRR_ID_CODE'})\n",
    "    return rej\n",
    "\n",
    "rej = clean_data(rej.copy())\n",
    "rej.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_clean.merge(rej[[\"TRR_ID_CODE\", \"DID_REJECT\"]], on=\"TRR_ID_CODE\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(columns=\"TRR_ID_CODE\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_merged):\n",
    "    # Replace all instances of \"Y\" with \"1\" in column: 'DID_REJECT'\n",
    "    df_merged['DID_REJECT'] = df_merged['DID_REJECT'].str.replace(\"Y\", \"1\", case=False, regex=False)\n",
    "    # Replace all instances of \"N\" with \"0\" in column: 'DID_REJECT'\n",
    "    df_merged['DID_REJECT'] = df_merged['DID_REJECT'].str.replace(\"N\", \"0\", case=False, regex=False)\n",
    "    return df_merged\n",
    "\n",
    "df_merged = clean_data(df_merged.copy())\n",
    "df_merged.head()\n",
    "\n",
    "df_merged = df_merged.astype({'GENDER': 'float64', 'BMI_RECIP': 'float64', 'HGT_CM_TRR': 'float64', 'WGT_KG_TRR': 'float64', 'AGE': 'float64', 'DA1': 'float64', 'DA2': 'float64', 'DB1': 'float64', 'DB2': 'float64', 'DDR1': 'float64', 'DDR2': 'float64', 'DON_TY': 'float64', 'MULTIORG': 'float64', 'TXINT': 'float64', 'TXKID': 'float64', 'TXLIV': 'float64', 'TXPAN': 'float64', 'TXVCA': 'float64', 'MULTIVCA': 'float64', 'PREV_TX_ANY': 'float64', 'PSTATUS': 'float64', 'PTIME': 'float64', 'DISTANCE': 'float64', 'BMI_DON_CALC': 'float64', 'WGT_KG_DON_CALC': 'float64', 'HGT_CM_DON_CALC': 'float64', 'CARDARREST_POSTNEURO_DON': 'float64', 'GENDER_DON': 'float64', 'AGE_DON': 'float64', 'SERUM_CREAT': 'float64', 'EXTRA_ALLOGRAFT': 'float64', '5_year': 'float64', 'ABO_A': 'float64', 'ABO_AB': 'float64', 'ABO_B': 'float64', 'ABO_O': 'float64', 'ETHCAT_1': 'float64', 'ETHCAT_2': 'float64', 'ETHCAT_3': 'float64', 'ETHCAT_4': 'float64', 'ETHCAT_5': 'float64', 'ETHCAT_7': 'float64', 'ETHCAT_9': 'float64', 'DIAG_1': 'float64', 'DIAG_2': 'float64', 'DIAG_3': 'float64', 'DIAG_4': 'float64', 'DIAG_5': 'float64', 'DIAG_6': 'float64', 'DIAG_8': 'float64', 'DIAG_9': 'float64', 'VCA_TY_Abdominal Wall': 'float64', 'VCA_TY_Face': 'float64', 'VCA_TY_Larynx': 'float64', 'VCA_TY_Penis': 'float64', 'VCA_TY_Scalp': 'float64', 'VCA_TY_Trachea': 'float64', 'VCA_TY_Upper Limb Bilateral': 'float64', 'VCA_TY_Upper Limb Left': 'float64', 'VCA_TY_Upper Limb Right': 'float64', 'VCA_TY_Uterus': 'float64', 'INO_PROCURE_AGENT_1_1': 'float64', 'INO_PROCURE_AGENT_1_2': 'float64', 'INO_PROCURE_AGENT_1_3': 'float64', 'INO_PROCURE_AGENT_1_4': 'float64', 'INO_PROCURE_AGENT_1_5': 'float64', 'INO_PROCURE_AGENT_1_7': 'float64', 'INO_PROCURE_AGENT_1_0': 'float64', 'INO_PROCURE_AGENT_2_1': 'float64', 'INO_PROCURE_AGENT_2_4': 'float64', 'INO_PROCURE_AGENT_2_5': 'float64', 'INO_PROCURE_AGENT_2_7': 'float64', 'INO_PROCURE_AGENT_2_0': 'float64', 'ARGININE_DON_N': 'float64', 'ARGININE_DON_U': 'float64', 'ARGININE_DON_Y': 'float64', 'INSULIN_DON_N': 'float64', 'INSULIN_DON_U': 'float64', 'INSULIN_DON_Y': 'float64', 'TATTOOS_DON_N': 'float64', 'TATTOOS_DON_U': 'float64', 'TATTOOS_DON_Y': 'float64', 'PROTEIN_URINE_DON_N': 'float64', 'PROTEIN_URINE_DON_U': 'float64', 'PROTEIN_URINE_DON_Y': 'float64', 'INOTROP_SUPPORT_DON_N': 'float64', 'INOTROP_SUPPORT_DON_U': 'float64', 'INOTROP_SUPPORT_DON_Y': 'float64', 'ABO_DON_A': 'float64', 'ABO_DON_A1': 'float64', 'ABO_DON_A2': 'float64', 'ABO_DON_AB': 'float64', 'ABO_DON_B': 'float64', 'ABO_DON_O': 'float64', 'ETHCAT_DON_1': 'float64', 'ETHCAT_DON_2': 'float64', 'ETHCAT_DON_4': 'float64', 'CMV_STATUS_1': 'float64', 'CMV_STATUS_2': 'float64', 'CMV_STATUS_4': 'float64', 'EBV_SEROSTATUS_1': 'float64', 'EBV_SEROSTATUS_2': 'float64', 'EBV_SEROSTATUS_4': 'float64', 'TOLER_IND_TECH_N': 'float64', 'TOLER_IND_TECH_U': 'float64', 'TOLER_IND_TECH_Y': 'float64', 'PRE_TX_TXFUS_N': 'float64', 'PRE_TX_TXFUS_U': 'float64', 'PRE_TX_TXFUS_Y': 'float64', 'OTH_RISK_FACTORS_N': 'float64', 'OTH_RISK_FACTORS_U': 'float64', 'OTH_RISK_FACTORS_Y': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_merged.pop(\"DID_REJECT\").astype(\"int64\")\n",
    "df_merged = df_merged.drop(columns=['PTIME', '5_year', 'HGT_CM_TRR', 'WGT_KG_TRR', 'HGT_CM_DON_CALC', 'WGT_KG_DON_CALC', 'PSTATUS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['DON_TY', 'DISTANCE', 'AGE_DON', 'DIAG_1', 'DIAG_5',\n",
      "       'VCA_TY_Upper Limb Left', 'VCA_TY_Uterus', 'TATTOOS_DON_N',\n",
      "       'TATTOOS_DON_U', 'PROTEIN_URINE_DON_N', 'PROTEIN_URINE_DON_U',\n",
      "       'INOTROP_SUPPORT_DON_N', 'ETHCAT_DON_1', 'ETHCAT_DON_4',\n",
      "       'EBV_SEROSTATUS_2', 'EBV_SEROSTATUS_4', 'TOLER_IND_TECH_N',\n",
      "       'TOLER_IND_TECH_U', 'TOLER_IND_TECH_Y', 'PRE_TX_TXFUS_N',\n",
      "       'PRE_TX_TXFUS_U', 'PRE_TX_TXFUS_Y', 'OTH_RISK_FACTORS_N',\n",
      "       'OTH_RISK_FACTORS_U', 'OTH_RISK_FACTORS_Y'],\n",
      "      dtype='object')\n",
      "Accuracy with selected features: 0.6428571428571429\n",
      "5-Fold Cross-Validation Accuracy: 0.69 ± 0.10\n",
      "0.7230769230769231\n",
      "0.6428571428571429\n",
      "ROC_AUC: 0.65625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your data\n",
    "# X: Features, y: Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merged, result, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, max_leaf_nodes=2, random_state=42)\n",
    "\n",
    "# Initialize RFE with the Random Forest as the estimator\n",
    "rfe = RFE(estimator=rf, n_features_to_select=25)  # You can specify the number of features to select\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Check which features were selected\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Transform the dataset to the selected features\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Train the Random Forest on the selected features\n",
    "rf.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf.predict(X_test_rfe)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with selected features: {accuracy}\")\n",
    "\n",
    "cv_scores = cross_val_score(rf, X_train_rfe, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"5-Fold Cross-Validation Accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")\n",
    "\n",
    "print(rf.score(X_train_rfe, y_train))\n",
    "print(rf.score(X_test_rfe, y_test))\n",
    "\n",
    "preds = rf.predict(X_test_rfe)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "print(f\"ROC_AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests la7alo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merged, result, test_size=0.3, random_state=42)\n",
    "\n",
    "rsf = RandomForestClassifier(n_estimators=500, max_depth=6, max_leaf_nodes=2, random_state=42)\n",
    "rsf.fit(X_train, y_train)\n",
    "\n",
    "print(rsf.score(X_train, y_train))\n",
    "print(rsf.score(X_test, y_test))\n",
    "\n",
    "preds = rsf.predict(X_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "print(f\"ROC_AUC: {roc_auc}\")\n",
    "\n",
    "cv_scores = cross_val_score(rsf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"5-Fold Cross-Validation Accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ma zabat burrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data: replace X and y with your dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merged, result, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "# Initialize Boruta feature selection method\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "\n",
    "# Fit Boruta on the training data\n",
    "boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "# Check selected features\n",
    "selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "\n",
    "# Check the ranking of features\n",
    "ranking_features = X_train.columns[boruta_selector.ranking_].tolist()\n",
    "print(f\"Ranking of Features: {ranking_features}\")\n",
    "\n",
    "# Transform training data to only include selected features\n",
    "X_train_boruta = boruta_selector.transform(X_train.values)\n",
    "X_test_boruta = boruta_selector.transform(X_test.values)\n",
    "\n",
    "# Fit Random Forest on the selected features\n",
    "rf.fit(X_train_boruta, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = rf.score(X_test_boruta, y_test)\n",
    "print(f\"Accuracy after Boruta Feature Selection: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df_merged.columns\n",
    "# Get feature importances\n",
    "importances = rsf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Calculate Relative Variable Importance (RVI)\n",
    "feature_importance_df['RVI'] = feature_importance_df['Importance'] / feature_importance_df['Importance'].sum()\n",
    "\n",
    "# Calculate Cumulative RVI\n",
    "feature_importance_df['Cumulative RVI'] = feature_importance_df['RVI'].cumsum()\n",
    "\n",
    "# Sort by RVI in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='RVI', ascending=False)\n",
    "\n",
    "# Display the feature importance DataFrame with RVI and Cumulative RVI\n",
    "\n",
    "# Display the top contributing variables\n",
    "print(feature_importance_df.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna for random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 30)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 20)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                 min_samples_split=min_samples_split, \n",
    "                                 min_samples_leaf=min_samples_leaf,\n",
    "                                 max_features=max_features,\n",
    "                                 bootstrap=bootstrap,\n",
    "                                 max_leaf_nodes=max_leaf_nodes)\n",
    "    \n",
    "    score = cross_val_score(clf, X_train, y_train, n_jobs=-1, cv=5, scoring='roc_auc').mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# [I 2024-10-07 14:31:09,420] Trial 56 finished with value: 0.6923076923076923 and parameters: {'n_estimators': 262, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 15, 'max_features': 'log2', 'bootstrap': False, 'max_leaf_nodes': 6}. Best is trial 56 with value: 0.6923076923076923.\n",
    "\n",
    "# [I 2024-10-07 14:43:13,706] Trial 77 finished with value: 0.7523809523809524 and parameters: {'n_estimators': 450, 'max_depth': 27, 'min_samples_split': 15, 'min_samples_leaf': 16, 'max_features': 'sqrt', 'bootstrap': False, 'max_leaf_nodes': 5}. Best is trial 77 with value: 0.7523809523809524.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 30)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0, 5)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    scale_pos_weight = trial.suggest_float('scale_pos_weight', 1.0, 10.0)\n",
    "    \n",
    "    # Define the classifier with the hyperparameters\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma, \n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample, \n",
    "        colsample_bytree=colsample_bytree,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        use_label_encoder=False, # To suppress warnings from XGB\n",
    "        eval_metric='logloss'    # To suppress eval_metric warning\n",
    "    )\n",
    "    \n",
    "    # Use cross_val_score to calculate the average AUC across 5-fold cross-validation\n",
    "    score = cross_val_score(clf, X_train, y_train, scoring='accuracy', n_jobs=-1, cv=5).mean()\n",
    "    return score\n",
    "\n",
    "# Create the Optuna study to maximize the AUC\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "# Best hyperparameters: {'n_estimators': 666, 'max_depth': 7, 'learning_rate': 0.20371850795302843, 'gamma': 3.821025018002871, 'min_child_weight': 5, 'subsample': 0.7209854517618058, 'colsample_bytree': 0.5587085908542856, 'scale_pos_weight': 1.0009047312384416}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB msh shaghal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6307692307692307\n",
      "0.8461538461538461\n",
      "0.6428571428571429\n",
      "ROC_AUC: 0.6458333333333333\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier with the hyperparameters\n",
    "Best_hyperparameters =  {'n_estimators': 666, 'max_depth': 7, 'learning_rate': 0.20371850795302843, 'gamma': 3.821025018002871, \n",
    "                         'min_child_weight': 5, 'subsample': 0.7209854517618058, 'colsample_bytree': 0.5587085908542856, \n",
    "                         'scale_pos_weight': 1.0009047312384416}\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "  # Add L2 regularization\n",
    ")\n",
    "\n",
    "\n",
    "xgb_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "xgb_model.score(X_test_rfe, y_test)\n",
    "\n",
    "score = cross_val_score(xgb_model, X_train_rfe, y_train, scoring='accuracy', n_jobs=-1, cv=5).mean()\n",
    "\n",
    "print(score)\n",
    "\n",
    "print(xgb_model.score(X_train_rfe, y_train))\n",
    "print(xgb_model.score(X_test_rfe, y_test))\n",
    "\n",
    "preds = xgb_model.predict(X_test_rfe)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "print(f\"ROC_AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression time !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7846153846153846\n",
      "0.7142857142857143\n",
      "ROC_AUC: 0.7291666666666667\n",
      "5-Fold Cross-Validation Accuracy: 0.68 ± 0.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR =  LogisticRegression(max_iter=1000, random_state=42)\n",
    "LR.fit(X_train_rfe, y_train)\n",
    "LR.score(X_train_rfe, y_train)\n",
    "\n",
    "print(LR.score(X_train_rfe, y_train))\n",
    "print(LR.score(X_test_rfe, y_test))\n",
    "\n",
    "preds = LR.predict(X_test_rfe)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "print(f\"ROC_AUC: {roc_auc}\")\n",
    "\n",
    "cv_scores = cross_val_score(LR, X_train_rfe, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"5-Fold Cross-Validation Accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking time !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model Accuracy: 0.71\n",
      "Stacked Model ROC_AUC: 0.72\n",
      "5-Fold Cross-Validation Accuracy: 0.69 ± 0.04\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.69      0.73        16\n",
      "           1       0.64      0.75      0.69        12\n",
      "\n",
      "    accuracy                           0.71        28\n",
      "   macro avg       0.71      0.72      0.71        28\n",
      "weighted avg       0.72      0.71      0.72        28\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  5]\n",
      " [ 3  9]]\n",
      "Sensitivity (Recall): 0.75\n",
      "Specificity: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "# Initialize base models\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, max_leaf_nodes=2, random_state=42)\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.01, max_depth=7, random_state=42)\n",
    "LR = LogisticRegression(max_iter=1000, random_state=42)\n",
    "Support = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Initialize StackingClassifier with the base models\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('SVC', Support),\n",
    "        ('rf', rf),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lr', LR)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Fit the Stacking Classifier\n",
    "stacked_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = stacked_model.predict(X_test_rfe)\n",
    "\n",
    "# Accuracy\n",
    "stacked_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacked Model Accuracy: {stacked_accuracy:.2f}\")\n",
    "\n",
    "# ROC AUC Score\n",
    "stacked_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"Stacked Model ROC_AUC: {stacked_roc_auc:.2f}\")\n",
    "\n",
    "# 5-Fold Cross-Validation Accuracy\n",
    "cv_scores = cross_val_score(stacked_model, X_train_rfe, y_train, cv=3, scoring='accuracy')\n",
    "print(f\"5-Fold Cross-Validation Accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Extracting values from the confusion matrix\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "sensitivity = TP / (TP + FN)\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.2f}\")\n",
    "\n",
    "# Specificity\n",
    "specificity = TN / (TN + FP)\n",
    "print(f\"Specificity: {specificity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for imputing the missing PTIME values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_imputation_models(df, target_column='PTIME', top_features=20):  # Set top_features to 5\n",
    "    # Step 1: Separate data with and without missing PTIME values\n",
    "    df_not_missing = df[df[target_column].notna()]\n",
    "    df_missing = df[df[target_column].isna()]\n",
    "    \n",
    "    # Step 2: Separate features and target\n",
    "    X_train = df_not_missing.drop(columns=[target_column])\n",
    "    y_train = df_not_missing[target_column]\n",
    "    \n",
    "    X_missing = df_missing.drop(columns=[target_column])\n",
    "    \n",
    "    # Step 3: Feature Scaling (MinMaxScaler)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Step 4: Train a Random Forest to determine feature importance\n",
    "    rf_selector_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_selector_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Use SelectFromModel to select the top 5 most important features\n",
    "    selector = SelectFromModel(rf_selector_model, max_features=top_features, prefit=True)\n",
    "    \n",
    "    # Get the selected feature names\n",
    "    feature_names = X_train.columns  # Original feature names\n",
    "    selected_features = feature_names[selector.get_support()]  # Get the selected feature names\n",
    "    \n",
    "    print(\"Top 5 selected features:\")\n",
    "    print(selected_features)\n",
    "\n",
    "    # Select the same top features from the scaled training set\n",
    "    X_train_scaled_selected = selector.transform(X_train_scaled)\n",
    "    \n",
    "    # Select the same top features from X_missing if needed (if you're imputing later)\n",
    "    if not X_missing.empty:\n",
    "        X_missing_scaled = scaler.transform(X_missing)\n",
    "        X_missing_scaled_selected = selector.transform(X_missing_scaled)\n",
    "    \n",
    "    # Split train data for validation purposes\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_scaled_selected, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize a dictionary to store model predictions\n",
    "    validation_predictions = {}\n",
    "\n",
    "    # Step 5: Train and validate models with the top 5 features\n",
    "    # 5.1 Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_split, y_train_split)\n",
    "    rf_pred_val = rf_model.predict(X_val_split)\n",
    "    \n",
    "    # Calculate MAPE for Random Forest\n",
    "    rf_mape = mean_absolute_percentage_error(y_val_split, rf_pred_val) * 100\n",
    "    print(\"Random Forest Validation MAPE: {:.2f}%\".format(rf_mape))\n",
    "    validation_predictions['RandomForest'] = rf_pred_val\n",
    "\n",
    "    # 5.2 XGBoost Regressor\n",
    "    xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    xgb_model.fit(X_train_split, y_train_split)\n",
    "    xgb_pred_val = xgb_model.predict(X_val_split)\n",
    "    \n",
    "    # Calculate MAPE for XGBoost\n",
    "    xgb_mape = mean_absolute_percentage_error(y_val_split, xgb_pred_val) * 100\n",
    "    print(\"XGBoost Validation MAPE: {:.2f}%\".format(xgb_mape))\n",
    "    validation_predictions['XGBoost'] = xgb_pred_val\n",
    "\n",
    "    # 5.3 MLP Regressor (Neural Network)\n",
    "    mlp_model = MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=42)\n",
    "    mlp_model.fit(X_train_split, y_train_split)\n",
    "    mlp_pred_val = mlp_model.predict(X_val_split)\n",
    "    \n",
    "    # Calculate MAPE for MLP\n",
    "    mlp_mape = mean_absolute_percentage_error(y_val_split, mlp_pred_val) * 100\n",
    "    print(\"MLP Validation MAPE: {:.2f}%\".format(mlp_mape))\n",
    "    validation_predictions['MLP'] = mlp_pred_val\n",
    "    \n",
    "    # Step 6: Choose the model with the best performance based on MAPE\n",
    "    best_model_name = min(validation_predictions, key=lambda k: mean_absolute_percentage_error(y_val_split, validation_predictions[k]))\n",
    "    \n",
    "    print(f\"The best model based on validation MAPE is: {best_model_name}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to the cleaned DataFrame and select the top 5 features\n",
    "evaluate_imputation_models(df_clean, top_features=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
